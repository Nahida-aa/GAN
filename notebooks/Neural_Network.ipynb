{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络（Neural Network）是一种模拟生物神经系统的计算模型，广泛应用于机器学习和人工智能领域。神经网络由多个神经元（Neuron）组成，这些神经元通过连接（Connection）形成网络结构。以下是神经网络的基本概念和原理。\n",
    "\n",
    "### 神经网络的基本概念\n",
    "\n",
    "1. **神经元（Neuron）**：\n",
    "   - 神经元是神经网络的基本单位，类似于生物神经元。\n",
    "   - 每个神经元接收多个输入信号，通过加权求和和激活函数处理后，输出一个信号。\n",
    "\n",
    "2. **权重（Weight）**：\n",
    "   - 权重是连接神经元的参数，表示输入信号的重要性。\n",
    "   - 权重在训练过程中不断调整，以最小化损失函数。\n",
    "\n",
    "3. **偏置（Bias）**：\n",
    "   - 偏置是一个额外的参数，用于调整神经元的输出。\n",
    "   - 偏置在训练过程中也会不断调整。\n",
    "\n",
    "4. **激活函数（Activation Function）**：\n",
    "   - 激活函数用于引入非线性，使神经网络能够处理复杂的非线性问题。\n",
    "   - 常见的激活函数包括 Sigmoid、ReLU（Rectified Linear Unit）、Tanh 等。\n",
    "\n",
    "5. **层（Layer）**：\n",
    "   - 神经网络由多个层组成，每层包含多个神经元。\n",
    "   - 输入层（Input Layer）：接收输入数据。\n",
    "   - 隐藏层（Hidden Layer）：处理输入数据，提取特征。\n",
    "   - 输出层（Output Layer）：输出预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络的工作原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.前向传播 (Forward Propagation)\n",
    "在前向传播过程中，输入数据通过网络的各层传递，最终生成输出结果\n",
    "\n",
    "1. **输入数据传递到输入层**：\n",
    "   - 输入数据（例如，MNIST 图像展平后的向量）作为输入层的输入。\n",
    "\n",
    "2. **输入层到隐藏层的传递**：\n",
    "   - 输入层的输出（即输入数据）传递到第一个隐藏层。\n",
    "   - 每个隐藏层的神经元计算输入信号的加权和，并通过激活函数处理后输出信号。\n",
    "\n",
    "3. **隐藏层到输出层的传递**：\n",
    "   - 隐藏层的输出传递到输出层。\n",
    "   - 输出层的神经元计算输入信号的加权和，并通过激活函数处理后输出信号。\n",
    "\n",
    "4. **输出层的输出**：\n",
    "   - 输出层的输出即为神经网络的预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 数学表示\n",
    "假设我们有一个简单的两层神经网络，输入层有 $ n $ 个神经元，隐藏层有 $ h $ 个神经元，输出层有 $ m $ 个神经元。\n",
    "1. **输入层到隐藏层**：\n",
    "   - 输入数据： $ \\mathbf{x} \\in \\mathbb{R}^n $\n",
    "   - 权重矩阵： $ \\mathbf{W}_1 \\in \\mathbb{R}^{h \\times n} $\n",
    "   - 偏置向量： $ \\mathbf{b}_1 \\in \\mathbb{R}^h $\n",
    "   - 激活函数： $ \\sigma(\\cdot) , \\cdot \\in \\mathbb{R}^h $\n",
    "   - 隐藏层输出： $ \\mathbf{h} = \\sigma(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1) $\n",
    "\n",
    "2. **隐藏层到输出层**：\n",
    "   - 权重矩阵： $ \\mathbf{W}_2 \\in \\mathbb{R}^{m \\times h} $\n",
    "   - 偏置向量： $ \\mathbf{b}_2 \\in \\mathbb{R}^m $\n",
    "   - 激活函数： $ \\sigma(\\cdot) , \\cdot \\in \\mathbb{R}^m $\n",
    "   - 输出层输出： $ \\mathbf{y} = \\sigma(\\mathbf{W}_2 \\mathbf{h} + \\mathbf{b}_2) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 具体实现\n",
    "以下是使用 PyTorch 实现前向传播的示例代码。\n",
    "\n",
    "###### 定义神经网络模型\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 输入层到隐藏层的全连接层\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # 隐藏层到输出层的全连接层\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)  # 输入层到隐藏层\n",
    "        out = self.relu(out)  # 激活函数\n",
    "        out = self.fc2(out)  # 隐藏层到输出层\n",
    "        return out\n",
    "```\n",
    "\n",
    "###### 前向传播示例\n",
    "```python\n",
    "# 超参数\n",
    "input_size = 784  # 输入层大小（28x28 的图像展平为向量）\n",
    "hidden_size = 256  # 隐藏层大小\n",
    "output_size = 10  # 输出层大小（10 个类别）\n",
    "\n",
    "# 初始化模型\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# 示例输入数据（一个批次的 MNIST 图像展平后的向量）\n",
    "input_data = torch.randn(64, input_size)  # 64 是批次大小\n",
    "\n",
    "# 前向传播\n",
    "output = model(input_data)\n",
    "\n",
    "print(output.shape)  # 输出的形状应为 (64, 10)\n",
    "```\n",
    "\n",
    "##### 解释\n",
    "1. **定义神经网络模型**：\n",
    "   - `self.fc1 = nn.Linear(input_size, hidden_size)`：定义输入层到隐藏层的全连接层。\n",
    "   - `self.relu = nn.ReLU()`：定义激活函数。\n",
    "   - `self.fc2 = nn.Linear(hidden_size, output_size)`：定义隐藏层到输出层的全连接层。\n",
    "\n",
    "2. **前向传播**：\n",
    "   - `out = self.fc1(x)`：输入数据通过输入层传递到隐藏层，计算加权和。\n",
    "   - `out = self.relu(out)`：隐藏层的输出通过激活函数处理。\n",
    "   - `out = self.fc2(out)`：隐藏层的输出传递到输出层，计算加权和。\n",
    "\n",
    "3. **示例输入数据**：\n",
    "   - `input_data = torch.randn(64, input_size)`：生成一个批次的随机输入数据，形状为 (64, 784)。\n",
    "   - `output = model(input_data)`：进行前向传播，计算输出结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.损失函数 (Loss Function)\n",
    "损失函数用于衡量神经网络的预测结果与真实值之间的差异。它是训练过程中优化的目标，神经网络通过最小化损失函数来调整其参数。\n",
    "##### 常见的损失函数\n",
    "1. **均方误差（Mean Squared Error, MSE）**：\n",
    "   - 主要用于回归问题。\n",
    "   - 公式：$$ \\text{MSE}(y,\\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 \\quad , \\quad y,\\hat{y} \\in \\mathbb{R}^N $$\n",
    "   - 其中，$ y_i $ 是真实值，$ \\hat{y}_i $ 是预测值，$ N $ 是样本数量。\n",
    "\n",
    "2. **交叉熵损失（Cross-Entropy Loss）**：\n",
    "   - 主要用于分类问题。\n",
    "   - 公式：$$ \\text{Cross-Entropy Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c}) $$\n",
    "   - 其中，$ y_{i,c} $ 是真实标签的 one-hot 编码，$ \\hat{y}_{i,c} $ 是预测概率，$ C $ 是类别数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### example\n",
    "\n",
    "###### 1. MSE\n",
    "\n",
    "假设我们有一个简单的回归问题，真实值和预测值如下：\n",
    "- 真实值 $ y = [2.0, 3.0, 4.0] $\n",
    "- 预测值 $ \\hat{y} = [2.5, 2.8, 4.2] $\n",
    "- 计算均方误差：\n",
    "   $$\n",
    "   \\text{MSE} = \\frac{1}{3} [(2.0 - 2.5)^2 + (3.0 - 2.8)^2 + (4.0 - 4.2)^2] = \\frac{0.33}{3} = 0.11\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Binary-Cross-Entropy Loss\n",
    "- C = 2 时的 Cross-Entropy Loss, 用于2分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "交叉熵损失主要用于分类问题，用于衡量预测概率分布与真实标签分布之间的差异。\n",
    "\n",
    "#### 例子\n",
    "假设我们有一个简单的分类问题，真实标签和预测概率如下：\n",
    "- 真实标签 $ y = [1, 0, 0] $（表示类别 0）\n",
    "- 预测概率 $ \\hat{y} = [0.7, 0.2, 0.1] $\n",
    "\n",
    "#### 计算过程\n",
    "1. 计算交叉熵损失：\n",
    "   $$\n",
    "   \\text{Cross-Entropy Loss} = - \\sum_{c=1}^{3} y_c \\log(\\hat{y}_c)\n",
    "   $$\n",
    "   其中，$ y_c $ 是真实标签的 one-hot 编码，$ \\hat{y}_c $ 是预测概率。\n",
    "\n",
    "2. 具体计算：\n",
    "   $$\n",
    "   \\text{Cross-Entropy Loss} = - (1 \\cdot \\log(0.7) + 0 \\cdot \\log(0.2) + 0 \\cdot \\log(0.1)) = - \\log(0.7) \\approx 0.357\n",
    "   $$\n",
    "\n",
    "### 代码示例\n",
    "以下是使用 PyTorch 实现均方误差和交叉熵损失的代码示例。\n",
    "\n",
    "#### 均方误差（MSE）示例\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 真实值和预测值\n",
    "y_true = torch.tensor([2.0, 3.0, 4.0])\n",
    "y_pred = torch.tensor([2.5, 2.8, 4.2])\n",
    "\n",
    "# 定义均方误差损失函数\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# 计算损失\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "print(f'MSE Loss: {loss.item():.4f}')\n",
    "```\n",
    "\n",
    "#### 交叉熵损失（Cross-Entropy Loss）示例\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 真实标签和预测概率\n",
    "y_true = torch.tensor([0])  # 类别 0\n",
    "y_pred = torch.tensor([[0.7, 0.2, 0.1]])  # 预测概率\n",
    "\n",
    "# 定义交叉熵损失函数\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 计算损失\n",
    "loss = cross_entropy_loss(y_pred, y_true)\n",
    "print(f'Cross-Entropy Loss: {loss.item():.4f}')\n",
    "```\n",
    "\n",
    "### 解释\n",
    "1. **均方误差（MSE）**：\n",
    "   - 计算每个样本的误差平方，并取平均值。\n",
    "   - 示例代码中，使用 PyTorch 的 `nn.MSELoss` 计算均方误差。\n",
    "\n",
    "2. **交叉熵损失（Cross-Entropy Loss）**：\n",
    "   - 计算预测概率分布与真实标签分布之间的差异。\n",
    "   - 示例代码中，使用 PyTorch 的 `nn.CrossEntropyLoss` 计算交叉熵损失。\n",
    "\n",
    "通过这种方式，你可以理解均方误差和交叉熵损失的计算过程和具体实现。如果你有任何问题，请随时提问。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. 反向传播（Backward Propagation）\n",
    "反向传播用于计算损失函数相对于每个参数的梯度。通过链式法则，梯度从输出层逐层传递到输入层。\n",
    "\n",
    "#### 反向传播的步骤\n",
    "1. **计算输出层的梯度**：\n",
    "   - 计算损失函数相对于输出层的梯度。\n",
    "\n",
    "2. **计算隐藏层的梯度**：\n",
    "   - 使用链式法则，将输出层的梯度传递到隐藏层，计算损失函数相对于隐藏层的梯度。\n",
    "\n",
    "3. **计算输入层的梯度**：\n",
    "   - 使用链式法则，将隐藏层的梯度传递到输入层，计算损失函数相对于输入层的梯度。\n",
    "\n",
    "### 4. 梯度下降（Gradient Descent）\n",
    "梯度下降用于更新神经网络的参数，以最小化损失函数。常见的梯度下降算法包括批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent, SGD）和小批量梯度下降（Mini-Batch Gradient Descent）。\n",
    "\n",
    "#### 梯度下降的步骤\n",
    "1. **计算梯度**：\n",
    "   - 使用反向传播计算损失函数相对于每个参数的梯度。\n",
    "\n",
    "2. **更新参数**：\n",
    "   - 使用梯度下降算法更新参数。\n",
    "   - 公式：\\[ \\theta = \\theta - \\eta \\nabla_\\theta J(\\theta) \\]\n",
    "   - 其中，\\( \\theta \\) 是参数，\\( \\eta \\) 是学习率，\\( \\nabla_\\theta J(\\theta) \\) 是损失函数相对于参数的梯度。\n",
    "\n",
    "### 具体实现\n",
    "以下是使用 PyTorch 实现损失函数、反向传播和梯度下降的示例代码。\n",
    "\n",
    "#### 定义神经网络模型\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 输入层到隐藏层的全连接层\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # 隐藏层到输出层的全连接层\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)  # 输入层到隐藏层\n",
    "        out = self.relu(out)  # 激活函数\n",
    "        out = self.fc2(out)  # 隐藏层到输出层\n",
    "        return out\n",
    "```\n",
    "\n",
    "#### 训练神经网络\n",
    "```python\n",
    "# 超参数\n",
    "input_size = 784  # 输入层大小（28x28 的图像展平为向量）\n",
    "hidden_size = 256  # 隐藏层大小\n",
    "output_size = 10  # 输出层大小（10 个类别）\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()  # 交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam 优化器\n",
    "\n",
    "# 加载数据\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='../data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 训练神经网络\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28*28)  # 将图像展平为向量\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward()  # 反向传播: 计算梯度\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "```\n",
    "\n",
    "### 解释\n",
    "1. **定义神经网络模型**：\n",
    "   - `self.fc1 = nn.Linear(input_size, hidden_size)`：定义输入层到隐藏层的全连接层。\n",
    "   - `self.relu = nn.ReLU()`：定义激活函数。\n",
    "   - `self.fc2 = nn.Linear(hidden_size, output_size)`：定义隐藏层到输出层的全连接层。\n",
    "\n",
    "2. **初始化模型、损失函数和优化器**：\n",
    "   - `criterion = nn.CrossEntropyLoss()`：定义交叉熵损失函数。\n",
    "   - `optimizer = optim.Adam(model.parameters(), lr=learning_rate)`：定义 Adam 优化器。\n",
    "\n",
    "3. **前向传播**：\n",
    "   - `outputs = model(images)`：输入数据通过网络的各层传递，生成输出结果。\n",
    "\n",
    "4. **计算损失**：\n",
    "   - `loss = criterion(outputs, labels)`：计算预测结果与真实值之间的差异。\n",
    "\n",
    "5. **反向传播和优化**：\n",
    "   - `optimizer.zero_grad()`：梯度清零，确保每次计算梯度时不会累加上一次计算的梯度。\n",
    "   - `loss.backward()`：反向传播，计算损失函数相对于每个参数的梯度。\n",
    "   - `optimizer.step()`：使用梯度下降算法更新参数。\n",
    "\n",
    "通过这种方式，你可以理解神经网络训练过程中的损失函数、反向传播和梯度下降的具体实现和发生的过程。如果你有任何问题，请随时提问。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **反向传播（Backward Propagation）**：\n",
    "   - 反向传播用于计算损失函数相对于每个权重和偏置的梯度。\n",
    "   - 通过链式法则，梯度从输出层逐层传递到输入层。\n",
    "\n",
    "4. **梯度下降（[Gradient Descent](./Gradient_Descent.ipynb)）**：\n",
    "   - 梯度下降用于更新权重和偏置，以最小化损失函数。\n",
    "   - 常见的梯度下降算法包括批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent, SGD）和小批量梯度下降（Mini-Batch Gradient Descent）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络的训练过程\n",
    "\n",
    "1. **初始化**：\n",
    "   - 初始化神经网络的权重和偏置。\n",
    "\n",
    "2. **前向传播**：\n",
    "   - 输入数据通过神经网络，计算输出结果。\n",
    "\n",
    "3. **计算损失**：\n",
    "   - 使用损失函数计算预测结果与真实值之间的差异。\n",
    "\n",
    "4. **反向传播**：\n",
    "   - 计算损失函数相对于每个权重和偏置的梯度。\n",
    "\n",
    "5. **更新参数**：\n",
    "   - 使用梯度下降算法更新权重和偏置。\n",
    "\n",
    "6. **重复**：\n",
    "   - 重复前向传播、计算损失、反向传播和更新参数的过程，直到损失函数收敛或达到预定的训练轮数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码示例: 用神经网络识别 MNIST 手写数字 的类别(标签, 0-9)\n",
    "\n",
    "对于 [MNIST 数据集](./MNIST_dataset.ipynb#MNIST-dataset-shape) 的单个图像的形状是 [1,28,28],展平为向量后形状为 [784], 作为神经网络的输入.\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1  \\\\\n",
    "x_2  \\\\\n",
    "\\vdots \\\\\n",
    "x_{783}  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "输入层: 784个输入神经元, 隐藏层: 建议选择:一层,256个神经元, 输出层: 10个神经元, 分别对应0-9的数字."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Loss: 0.4736\n",
      "Epoch [1/10], Step [200/938], Loss: 0.5091\n",
      "Epoch [1/10], Step [300/938], Loss: 0.3434\n",
      "Epoch [1/10], Step [400/938], Loss: 0.1720\n",
      "Epoch [1/10], Step [500/938], Loss: 0.2657\n",
      "Epoch [1/10], Step [600/938], Loss: 0.2019\n",
      "Epoch [1/10], Step [700/938], Loss: 0.3240\n",
      "Epoch [1/10], Step [800/938], Loss: 0.1952\n",
      "Epoch [1/10], Step [900/938], Loss: 0.2109\n",
      "Epoch [2/10], Step [100/938], Loss: 0.1176\n",
      "Epoch [2/10], Step [200/938], Loss: 0.1964\n",
      "Epoch [2/10], Step [300/938], Loss: 0.0366\n",
      "Epoch [2/10], Step [400/938], Loss: 0.1276\n",
      "Epoch [2/10], Step [500/938], Loss: 0.0917\n",
      "Epoch [2/10], Step [600/938], Loss: 0.0600\n",
      "Epoch [2/10], Step [700/938], Loss: 0.0919\n",
      "Epoch [2/10], Step [800/938], Loss: 0.0819\n",
      "Epoch [2/10], Step [900/938], Loss: 0.1962\n",
      "Epoch [3/10], Step [100/938], Loss: 0.0840\n",
      "Epoch [3/10], Step [200/938], Loss: 0.0656\n",
      "Epoch [3/10], Step [300/938], Loss: 0.0716\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0981\n",
      "Epoch [3/10], Step [500/938], Loss: 0.1767\n",
      "Epoch [3/10], Step [600/938], Loss: 0.0213\n",
      "Epoch [3/10], Step [700/938], Loss: 0.0828\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0316\n",
      "Epoch [3/10], Step [900/938], Loss: 0.1230\n",
      "Epoch [4/10], Step [100/938], Loss: 0.0348\n",
      "Epoch [4/10], Step [200/938], Loss: 0.0784\n",
      "Epoch [4/10], Step [300/938], Loss: 0.2223\n",
      "Epoch [4/10], Step [400/938], Loss: 0.1976\n",
      "Epoch [4/10], Step [500/938], Loss: 0.1757\n",
      "Epoch [4/10], Step [600/938], Loss: 0.1756\n",
      "Epoch [4/10], Step [700/938], Loss: 0.0523\n",
      "Epoch [4/10], Step [800/938], Loss: 0.1058\n",
      "Epoch [4/10], Step [900/938], Loss: 0.3588\n",
      "Epoch [5/10], Step [100/938], Loss: 0.0657\n",
      "Epoch [5/10], Step [200/938], Loss: 0.0303\n",
      "Epoch [5/10], Step [300/938], Loss: 0.0483\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0307\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0708\n",
      "Epoch [5/10], Step [600/938], Loss: 0.1276\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0351\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0107\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0297\n",
      "Epoch [6/10], Step [100/938], Loss: 0.0845\n",
      "Epoch [6/10], Step [200/938], Loss: 0.0817\n",
      "Epoch [6/10], Step [300/938], Loss: 0.1140\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0817\n",
      "Epoch [6/10], Step [500/938], Loss: 0.1191\n",
      "Epoch [6/10], Step [600/938], Loss: 0.1757\n",
      "Epoch [6/10], Step [700/938], Loss: 0.0468\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0738\n",
      "Epoch [6/10], Step [900/938], Loss: 0.1028\n",
      "Epoch [7/10], Step [100/938], Loss: 0.1353\n",
      "Epoch [7/10], Step [200/938], Loss: 0.0375\n",
      "Epoch [7/10], Step [300/938], Loss: 0.1014\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0283\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0651\n",
      "Epoch [7/10], Step [600/938], Loss: 0.0538\n",
      "Epoch [7/10], Step [700/938], Loss: 0.0614\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0211\n",
      "Epoch [7/10], Step [900/938], Loss: 0.1189\n",
      "Epoch [8/10], Step [100/938], Loss: 0.0320\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0145\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0075\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0394\n",
      "Epoch [8/10], Step [500/938], Loss: 0.0796\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0182\n",
      "Epoch [8/10], Step [700/938], Loss: 0.0119\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0382\n",
      "Epoch [8/10], Step [900/938], Loss: 0.1054\n",
      "Epoch [9/10], Step [100/938], Loss: 0.0215\n",
      "Epoch [9/10], Step [200/938], Loss: 0.0237\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0465\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0224\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0405\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0596\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0084\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0383\n",
      "Epoch [9/10], Step [900/938], Loss: 0.1515\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0229\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0909\n",
      "Epoch [10/10], Step [300/938], Loss: 0.1071\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0136\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0627\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0816\n",
      "Epoch [10/10], Step [700/938], Loss: 0.0366\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0091\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0079\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ../output/weights/simple_nn_model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[0;32m     71\u001b[0m simple_nn_model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../output/weights/simple_nn_model/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimple_nn_model_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msimple_nn_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# 6. 评估神经网络\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# 评估模型在测试集上的性能\u001b[39;00m\n\u001b[0;32m     75\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# 设置模型为评估模式\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\serialization.py:651\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    648\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\serialization.py:525\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    524\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\serialization.py:496\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory ../output/weights/simple_nn_model does not exist."
     ]
    }
   ],
   "source": [
    "# 1. 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. 定义数据预处理和加载数据\n",
    "# 定义数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图像转换为张量\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 归一化到 [-1, 1]\n",
    "])\n",
    "\n",
    "# 加载训练集和测试集\n",
    "train_dataset = datasets.MNIST(root='../data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='../data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 3. 定义神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# 超参数\n",
    "input_size = 784  # 输入层大小（28x28 的图像展平为向量）\n",
    "hidden_size = 256  # 隐藏层大小\n",
    "output_size = 10  # 输出层大小（10 个类别）\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "# 4. 初始化模型、损失函数和优化器\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# 5. 训练神经网络\n",
    "for epoch in range(epochs):\n",
    "    # 内层循环使用 train_loader 加载数据, 进行小批量的数据读取\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # if i == 3: # 为了快速验证代码的正确性，只迭代3次\n",
    "        #     break\n",
    "        # 内层每次迭代时，都会进行一次 梯度下降算法, 包括5个步骤\n",
    "        # 将图像展平为向量\n",
    "        # print(f\"batch_idx: {i}, images.shape: {images.shape}, labels.shape: {labels.shape}\")\n",
    "        # print(f\"labels: {labels}\")\n",
    "        images = images.view(-1, 28*28) # .shape = (64, 784)\n",
    "        # print(images.shape) \n",
    "        \n",
    "        outputs = model(images) # 1.前向传播: 计算输出\n",
    "        loss = criterion(outputs, labels) # 2.计算输出和标签之间的损失\n",
    "\n",
    "        # 这三步的顺序\n",
    "        optimizer.zero_grad() # 3.梯度清零: 以确保每次计算梯度时不会累加上一次计算的梯度\n",
    "        loss.backward() # 4.反向传播: 计算梯度\n",
    "        optimizer.step() # 5.更新参数\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "# 保存模型\n",
    "simple_nn_model_dir = '../output/weights/simple_nn_model/'\n",
    "torch.save(model.state_dict(), simple_nn_model_dir + 'simple_nn_model.pth')\n",
    "# 6. 评估神经网络\n",
    "# 评估模型在测试集上的性能\n",
    "model.eval()  # 设置模型为评估模式\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the 10000 test images: {100 * correct / total:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vits_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
